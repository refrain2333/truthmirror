"""
æ™ºèƒ½åˆ†ææœåŠ¡é›†æˆ
ç®€å•ç›´æ¥åœ°é›†æˆmodulesä¸‹çš„æ™ºèƒ½åˆ†æåŠŸèƒ½
"""

import os
import sys
import json
from pathlib import Path
from typing import Dict, Any
from datetime import datetime

# æ·»åŠ æ™ºèƒ½åˆ†æç³»ç»Ÿè·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
analysis_path = project_root / "modules" / "TruthNews" / "news_analysis"
sys.path.insert(0, str(analysis_path))

# è®¾ç½®å·¥ä½œç›®å½•ä¸ºåˆ†æç³»ç»Ÿç›®å½•
ANALYSIS_DIR = str(analysis_path)


def run_news_analysis(event_title: str, event_description: str, event_id: int = None) -> Dict[str, Any]:
    """
    è¿è¡Œæ–°é—»åˆ†æ
    
    Args:
        event_title: äº‹ä»¶æ ‡é¢˜
        event_description: äº‹ä»¶æè¿°
        
    Returns:
        åˆ†æç»“æœ
    """
    query = f"{event_title} {event_description}".strip()
    original_cwd = os.getcwd()
    
    # ç®€å•çŠ¶æ€å†™å…¥ï¼Œä¾›å‰ç«¯è½®è¯¢çœŸå®è¿›åº¦
    def _write_status(payload: Dict[str, Any]) -> None:
        try:
            status_file = Path(ANALYSIS_DIR) / f"status_event_{event_id}.json"
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
        except Exception:
            # çŠ¶æ€å†™å…¥å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            pass

    try:
        # é¦–å…ˆåˆ‡æ¢åˆ°åˆ†æç³»ç»Ÿç›®å½•
        os.chdir(ANALYSIS_DIR)
        
        # éªŒè¯æ™ºèƒ½åˆ†æç³»ç»Ÿæ˜¯å¦å¯ç”¨
        if not (Path(ANALYSIS_DIR) / "main.py").exists():
            raise FileNotFoundError("æ™ºèƒ½åˆ†æç³»ç»Ÿä¸»ç¨‹åºä¸å­˜åœ¨")
        
        # æ£€æŸ¥æ˜¯å¦é…ç½®äº†APIå¯†é’¥
        from dotenv import load_dotenv
        # å…ˆå°è¯•ä»åˆ†æç³»ç»Ÿç›®å½•åŠ è½½ç¯å¢ƒå˜é‡
        env_file = Path(ANALYSIS_DIR) / '.env'
        if env_file.exists():
            load_dotenv(env_file)
        else:
            load_dotenv()  # åŠ è½½é»˜è®¤çš„.env
        
        glm_api_key = os.getenv('GLM_API_KEY')
        print(f"è°ƒè¯•: GLM_API_KEY = {'å·²é…ç½®' if glm_api_key else 'æœªæ‰¾åˆ°'}")
        if not glm_api_key or glm_api_key == "è¯·åœ¨æ­¤å¤„å¡«å…¥æ‚¨çš„GLM APIå¯†é’¥":
            # å¦‚æœæ²¡æœ‰é…ç½®APIå¯†é’¥ï¼Œè¿”å›æç¤ºä¿¡æ¯
            return {
                "success": False,
                "error": "éœ€è¦é…ç½®GLM_API_KEYç¯å¢ƒå˜é‡",
                "query": query,
                "timestamp": datetime.now().isoformat(),
                "config_help": "è¯·é…ç½®ç¯å¢ƒå˜é‡ï¼šGLM_API_KEY=ä½ çš„APIå¯†é’¥"
            }
        
        print(f"ğŸ¤– [äº‹ä»¶{event_id}] å¯åŠ¨AIæ™ºèƒ½åˆ†æ: {query}")
        _write_status({
            "status": "started",
            "step": "step1_search",
            "event_id": event_id,
            "raw_count": 0,
            "started_at": datetime.now().isoformat()
        })
        
        # åŠ¨æ€å¯¼å…¥åˆ†æç³»ç»Ÿçš„ä¸»ç¨‹åº
        import sys
        if ANALYSIS_DIR not in sys.path:
            sys.path.insert(0, ANALYSIS_DIR)
        
        # ç¡®ä¿æ¯ä¸ªåˆ†æä»»åŠ¡æœ‰ç‹¬ç«‹çš„Pythonè·¯å¾„
        sys.path.insert(0, ANALYSIS_DIR)
        
        # å¯¼å…¥å¹¶è¿è¡ŒçœŸå®çš„åˆ†ææµç¨‹
        from main import run_news_analysis_pipeline
        
        print(f"ğŸ”„ [äº‹ä»¶{event_id}] æ‰§è¡ŒAIåˆ†ææµç¨‹")
        
        # è¿è¡Œå®Œæ•´çš„æ™ºèƒ½åˆ†ææµç¨‹
        try:
            # é‡è¦ï¼šæœç´¢åº”ä½¿ç”¨"çº¯æŸ¥è¯¢"ï¼Œä¸è¦æ‹¼æ¥äº‹ä»¶ID/æ—¶é—´æˆ³ä»¥å…å¯¼è‡´æœç´¢ç»“æœä¸º0
            print(f"ğŸ“ [äº‹ä»¶{event_id}] ä½¿ç”¨çº¯æŸ¥è¯¢è¿›è¡Œæœç´¢: {query}")
            result_file = run_news_analysis_pipeline(query)
            
            # è¯»å–æœ€ç»ˆåˆ†æç»“æœ
            if result_file and Path(result_file).exists():
                with open(result_file, 'r', encoding='utf-8') as f:
                    detailed_result = json.load(f)
                
                # æ ¹æ®AIåˆ†æç»“æœç”Ÿæˆè¯„çº§
                ai_reliability = generate_reliability_rating(detailed_result)

                # å†™å…¥å®ŒæˆçŠ¶æ€ï¼ˆåŒ…å«å…³é”®ç»Ÿè®¡ï¼‰å¹¶è¿›è¡Œæœ‰æ•ˆæ€§æ ¡éªŒ
                workflow = detailed_result.get("workflow_summary", {})
                raw_count = int(workflow.get("step1_raw_news") or 0)
                accessible = int(workflow.get("step2_accessible") or 0)
                relevant = int(workflow.get("step3_relevant") or 0)
                analyzed = int(workflow.get("step6_analyzed") or 0)

                if max(raw_count, accessible, relevant, analyzed) <= 0:
                    # æ— æ•°æ®æ—¶å°è¯•ç›´è¿AIå…œåº•
                    _write_status({
                        "status": "running",
                        "step": "step6_direct_ai_attempt",
                        "event_id": event_id,
                        "raw_count": raw_count,
                        "accessible": accessible,
                        "relevant": relevant,
                        "finished_at": datetime.now().isoformat()
                    })
                    # è§¦å‘æ— æ•°æ®å…œåº•ï¼šç›´æ¥è°ƒç”¨çœŸå®DeepSeekè¿›è¡Œæ–‡æœ¬åˆ†æ
                    fallback_summary = _direct_deepseek_analysis(query)
                    if fallback_summary:
                        _write_status({
                            "status": "completed",
                            "step": "finished",
                            "event_id": event_id,
                            "raw_count": raw_count,
                            "accessible": accessible,
                            "relevant": relevant,
                            "mode": "direct_ai",
                            "result_inline": {
                                "final_summary": fallback_summary,
                                "workflow_summary": workflow
                            },
                            "finished_at": datetime.now().isoformat()
                        })
                        return {
                            "success": True,
                            "event_id": event_id,
                            "query": query,
                            "ai_analysis": {
                                "reliability": "insufficient",  # æ— å¤–éƒ¨æ¥æºï¼Œä»…åŸºäºæ–‡æœ¬
                                "summary": fallback_summary,
                                "analysis_statistics": {"sources": 0, "mode": "direct_ai"},
                                "workflow_summary": workflow
                            },
                            "detailed_result": {
                                "final_summary": fallback_summary,
                                "workflow_summary": workflow
                            },
                            "result_file": result_file,
                            "timestamp": datetime.now().isoformat(),
                            "next_step": "REVIEW"
                        }
                    else:
                        return {
                            "success": False,
                            "event_id": event_id,
                            "query": query,
                            "error": "AIæœªè·å–åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·ç¨åé‡è¯•æˆ–æ›´æ¢å…³é”®è¯",
                            "ai_analysis": {
                                "reliability": "insufficient",
                                "summary": "æœ¬æ¬¡æŠ“å–å’Œæå–å‡æœªè·å¾—æœ‰æ•ˆå†…å®¹ï¼Œå»ºè®®è°ƒæ•´äº‹ä»¶æ ‡é¢˜/æè¿°åå†æ¬¡åˆ†æã€‚",
                                "analysis_statistics": detailed_result.get("analysis_statistics", {}),
                                "workflow_summary": workflow
                            },
                            "detailed_result": detailed_result,
                            "result_file": result_file,
                            "timestamp": datetime.now().isoformat(),
                            "next_step": "RETRY"
                        }

                _write_status({
                    "status": "completed",
                    "step": "step7_summary",
                    "event_id": event_id,
                    "raw_count": raw_count,
                    "accessible": accessible,
                    "relevant": relevant,
                    "result_file": str(Path(result_file).resolve()),
                    "finished_at": datetime.now().isoformat()
                })

                return {
                    "success": True,
                    "event_id": event_id,
                    "query": query,
                    "ai_analysis": {
                        "reliability": ai_reliability,
                        "summary": detailed_result.get("final_summary", ""),
                        "analysis_statistics": detailed_result.get("analysis_statistics", {}),
                        "workflow_summary": workflow
                    },
                    "detailed_result": detailed_result,
                    "result_file": result_file,
                    "timestamp": datetime.now().isoformat(),
                    "next_step": "VOTING"
                }
            else:
                raise FileNotFoundError("åˆ†æç»“æœæ–‡ä»¶æœªç”Ÿæˆ")
        
        except Exception as analysis_error:
            print(f"âŒ å®Œæ•´åˆ†ææµç¨‹å¤±è´¥: {analysis_error}")

            # é’ˆå¯¹å¸¸è§æ–‡ä»¶ç¼ºå¤±/æŠ“å–ä¸º0çš„æƒ…å†µï¼Œç›´æ¥å¯ç”¨çœŸå®AIå…œåº•ï¼Œé¿å…å‰ç«¯å¡æ­»
            err_text = str(analysis_error)
            if "No such file or directory" in err_text or "processed_data/04_raw_html_pages" in err_text:
                fallback_summary = _direct_deepseek_analysis(query)
                if fallback_summary:
                    _write_status({
                        "status": "completed",
                        "step": "step7_summary",
                        "event_id": event_id,
                        "mode": "direct_ai",
                        "result_inline": {
                            "final_summary": fallback_summary,
                            "workflow_summary": {"status": "direct_ai"}
                        },
                        "finished_at": datetime.now().isoformat()
                    })
                    return {
                        "success": True,
                        "event_id": event_id,
                        "query": query,
                        "ai_analysis": {
                            "reliability": "insufficient",
                            "summary": fallback_summary,
                            "analysis_statistics": {"sources": 0, "mode": "direct_ai"},
                            "workflow_summary": {"status": "direct_ai"}
                        },
                        "detailed_result": {"final_summary": fallback_summary},
                        "timestamp": datetime.now().isoformat(),
                        "next_step": "REVIEW"
                    }

            _write_status({
                "status": "failed",
                "step": "exception",
                "event_id": event_id,
                "error": str(analysis_error),
                "finished_at": datetime.now().isoformat()
            })

            # å¦‚æœå®Œæ•´åˆ†æå¤±è´¥ï¼Œè¿”å›åŸºç¡€åˆ†æç»“æœ
            return {
                "success": False,
                "event_id": event_id,
                "query": query,
                "error": f"AIåˆ†æè¿‡ç¨‹å‡ºé”™: {str(analysis_error)}",
                "error_type": "glm_api_error" if "409" in str(analysis_error) else "analysis_error",
                "suggestion": "è¯·æ£€æŸ¥GLM/DeepSeeké…ç½®æˆ–ç¨åé‡è¯•",
                "ai_analysis": {
                    "reliability": "insufficient",
                    "summary": f"ç”±äºæŠ€æœ¯é—®é¢˜ï¼Œæ— æ³•å®Œæˆå¯¹'{query}'çš„æ·±åº¦åˆ†æã€‚å»ºè®®æ‰‹åŠ¨éªŒè¯ç›¸å…³ä¿¡æ¯ã€‚",
                    "analysis_statistics": {"error": "analysis_failed"},
                    "workflow_summary": {"status": "failed", "error": str(analysis_error)[:200]}
                },
                "timestamp": datetime.now().isoformat(),
                "next_step": "MANUAL_REVIEW"
            }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "query": query,
            "timestamp": datetime.now().isoformat()
        }
    finally:
        # æ¢å¤åŸå·¥ä½œç›®å½•
        os.chdir(original_cwd)


def generate_reliability_rating(analysis_result: Dict[str, Any]) -> str:
    """
    æ ¹æ®AIåˆ†æç»“æœç”Ÿæˆå¯é æ€§è¯„çº§
    
    Args:
        analysis_result: AIåˆ†æç»“æœ
        
    Returns:
        str: å¯é æ€§è¯„çº§ (reliable/questionable/unreliable/insufficient)
    """
    try:
        # è·å–åˆ†æç»Ÿè®¡ä¿¡æ¯
        stats = analysis_result.get("analysis_statistics", {})
        total_items = stats.get("total_items", 0)
        successful_analyses = stats.get("successful_analyses", 0)
        
        # è®¡ç®—æˆåŠŸç‡
        success_rate = successful_analyses / total_items if total_items > 0 else 0
        
        # è·å–å·¥ä½œæµæ±‡æ€»
        workflow = analysis_result.get("workflow_summary", {})
        relevant_news = workflow.get("step3_relevant", 0)
        analyzed_count = workflow.get("step6_analyzed", 0)
        
        # è¯„çº§é€»è¾‘
        if total_items >= 10 and success_rate >= 0.8 and relevant_news >= 5:
            return "reliable"  # å¯é ï¼šä¿¡æ¯å……è¶³ä¸”åˆ†ææˆåŠŸç‡é«˜
        elif total_items >= 5 and success_rate >= 0.6 and relevant_news >= 3:
            return "questionable"  # å­˜ç–‘ï¼šä¿¡æ¯ä¸€èˆ¬ï¼Œéœ€è¦è¿›ä¸€æ­¥éªŒè¯
        elif total_items >= 3 and analyzed_count > 0:
            return "unreliable"  # ä¸å¯é ï¼šä¿¡æ¯è¾ƒå°‘æˆ–è´¨é‡ä¸é«˜
        else:
            return "insufficient"  # ä¿¡æ¯ä¸è¶³ï¼šæ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ†æ
            
    except Exception as e:
        print(f"è¯„çº§ç”Ÿæˆé”™è¯¯: {e}")
        return "insufficient"


def get_analysis_result(result_file: str) -> Dict[str, Any]:
    """
    è·å–åˆ†æç»“æœå†…å®¹
    
    Args:
        result_file: ç»“æœæ–‡ä»¶è·¯å¾„
        
    Returns:
        åˆ†æç»“æœå†…å®¹
    """
    try:
        import json
        with open(result_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        return {"error": f"è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥: {e}"}


# ç›´æ¥è°ƒç”¨DeepSeekä½œä¸ºå…œåº•ï¼ˆçœŸå®AIï¼‰
def _direct_deepseek_analysis(text: str) -> str | None:
    try:
        import os, requests
        api_key = os.getenv('DEEPSEEK_API_KEY')
        model_id = os.getenv('DEEPSEEK_MODEL_ID', 'deepseek-chat')
        if not api_key:
            return None
        url = 'https://api.deepseek.com/chat/completions'
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        payload = {
            'model': model_id,
            'messages': [
                { 'role': 'system', 'content': 'You are an investigative news analyst. Provide concise, structured conclusions with caveats when sources are missing.' },
                { 'role': 'user', 'content': f"æ ¹æ®ä¸‹é¢äº‹ä»¶æ–‡æœ¬è¿›è¡Œäº‹å®æ ¸éªŒå–å‘çš„åˆ†æã€‚è¦æ±‚ï¼š\n- ç»™å‡ºå…³é”®äº‹å®æ¸…å•\n- æ ‡æ³¨ä¸ç¡®å®šç‚¹\n- ç»™å‡ºè¿›ä¸€æ­¥æ ¸éªŒå»ºè®®\n\näº‹ä»¶æ–‡æœ¬ï¼š{text}" }
            ],
            'temperature': 0.3,
            'max_tokens': 800
        }
        resp = requests.post(url, headers=headers, json=payload, timeout=30)
        if resp.status_code == 200:
            data = resp.json()
            return data.get('choices', [{}])[0].get('message', {}).get('content')
        return None
    except Exception:
        return None
