#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ­¥éª¤7: ç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
æ•´åˆæ‰€æœ‰åˆ†æç»“æœï¼Œç”Ÿæˆç»¼åˆæ€§æ€»ç»“æŠ¥å‘Š
"""

import requests
import json
import os
import glob
from datetime import datetime
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def call_glm_summary_api(query, all_analyzed_texts):
    """
    è°ƒç”¨GLM APIç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š

    Args:
        query (str): æœç´¢å…³é”®è¯
        all_analyzed_texts (str): æ‰€æœ‰åˆ†ææ–‡æœ¬çš„æ‹¼æ¥

    Returns:
        tuple: (success, summary_result, error_message)
    """
    api_url = os.getenv('GLM_API_URL', 'https://open.bigmodel.cn/api/paas/v4/')
    api_key = os.getenv('GLM_API_KEY')
    model_id = os.getenv('GLM_MODEL_ID', 'glm-4.5-flash')

    if not api_key:
        raise ValueError("GLM_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®")

    # è·å–æ±‡æ€»æç¤ºè¯æ¨¡æ¿
    summary_prompt_template = os.getenv('GLM_SUMMARY_PROMPT',
        'ä½ æ˜¯ä¸€ä½èµ„æ·±æ–°é—»åˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹å…³äº\'{query}\'çš„æ–°é—»åˆ†æï¼Œæ’°å†™ä¸€ä»½ç»¼åˆæ€§æ€»ç»“æŠ¥å‘Šã€‚\n\nåˆ†æå†…å®¹ï¼š\n{analyzed_content}')

    # æ„å»ºå®Œæ•´çš„æç¤ºè¯
    full_prompt = summary_prompt_template.format(
        query=query,
        analyzed_content=all_analyzed_texts
    )

    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }

    data = {
        'model': model_id,
        'messages': [
            {
                'role': 'user',
                'content': full_prompt
            }
        ],
        'temperature': 0.8,
        'max_tokens': 3000
    }
    
    try:
        # æ„å»ºå®Œæ•´çš„API URL
        full_url = f"{api_url.rstrip('/')}/chat/completions"

        response = requests.post(
            full_url,
            headers=headers,
            json=data,
            timeout=60
        )
        response.raise_for_status()

        result = response.json()

        # è§£æGLM APIçš„å“åº”æ ¼å¼
        if 'choices' in result and len(result['choices']) > 0:
            choice = result['choices'][0]
            if 'message' in choice and 'content' in choice['message']:
                summary_text = choice['message']['content']
                return True, summary_text.strip(), None
            else:
                return False, None, f"APIå“åº”æ ¼å¼å¼‚å¸¸: {result}"
        else:
            return False, None, f"APIè¿”å›æ— æœ‰æ•ˆé€‰æ‹©ç»“æœ: {result}"

    except requests.exceptions.Timeout:
        return False, None, "APIè¯·æ±‚è¶…æ—¶"
    except requests.exceptions.RequestException as e:
        return False, None, f"APIè¯·æ±‚å¤±è´¥: {str(e)}"
    except json.JSONDecodeError as e:
        return False, None, f"APIå“åº”JSONè§£æå¤±è´¥: {str(e)}"
    except Exception as e:
        return False, None, f"æœªçŸ¥é”™è¯¯: {str(e)}"


def call_deepseek_summary_api(query, all_analyzed_texts):
    """
    è°ƒç”¨DeepSeek APIç”Ÿæˆæ±‡æ€»æŠ¥å‘Šï¼ˆGLMå¤‡é€‰æ–¹æ¡ˆï¼‰
    
    Args:
        query (str): æœç´¢å…³é”®è¯
        all_analyzed_texts (str): æ‰€æœ‰åˆ†ææ–‡æœ¬
    
    Returns:
        tuple: (success, summary_result, error_message)
    """
    api_url = "https://api.deepseek.com/v1/chat/completions"
    api_key = os.getenv('DEEPSEEK_API_KEY')
    model_id = os.getenv('DEEPSEEK_MODEL_ID', 'deepseek-chat')

    if not api_key:
        return False, None, "DEEPSEEK_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®"

    # ä½¿ç”¨ç±»ä¼¼çš„æç¤ºè¯æ¨¡æ¿
    summary_prompt_template = os.getenv('DEEPSEEK_SUMMARY_PROMPT',
        'ä½ æ˜¯ä¸€ä½èµ„æ·±æ–°é—»åˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹å…³äº\'{query}\'çš„æ–°é—»åˆ†æï¼Œæ’°å†™ä¸€ä»½ç»¼åˆæ€§æ€»ç»“æŠ¥å‘Šã€‚\n\nåˆ†æå†…å®¹ï¼š\n{analyzed_content}')

    # æ„å»ºå®Œæ•´çš„æç¤ºè¯
    full_prompt = summary_prompt_template.format(
        query=query,
        analyzed_content=all_analyzed_texts
    )

    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }

    data = {
        'model': model_id,
        'messages': [
            {
                'role': 'user',
                'content': full_prompt
            }
        ],
        'temperature': 0.8,
        'max_tokens': 3000
    }
    
    try:
        response = requests.post(
            api_url,
            headers=headers,
            json=data,
            timeout=60
        )
        response.raise_for_status()
        
        result = response.json()
        
        if 'choices' in result and len(result['choices']) > 0:
            choice = result['choices'][0]
            if 'message' in choice and 'content' in choice['message']:
                summary_text = choice['message']['content']
                return True, summary_text.strip(), None
            else:
                return False, None, f"DeepSeek APIå“åº”æ ¼å¼å¼‚å¸¸: {result}"
        else:
            return False, None, f"DeepSeek APIè¿”å›æ— æœ‰æ•ˆé€‰æ‹©ç»“æœ: {result}"

    except requests.exceptions.Timeout:
        return False, None, "DeepSeek APIè¯·æ±‚è¶…æ—¶"
    except requests.exceptions.RequestException as e:
        return False, None, f"DeepSeek APIè¯·æ±‚å¤±è´¥: {str(e)}"
    except json.JSONDecodeError as e:
        return False, None, f"DeepSeek APIå“åº”JSONè§£æå¤±è´¥: {str(e)}"
    except Exception as e:
        return False, None, f"DeepSeek APIè°ƒç”¨æœªçŸ¥é”™è¯¯: {str(e)}"

def load_analyzed_data(query):
    """
    åŠ è½½æ­¥éª¤6çš„AIåˆ†æç»“æœ
    
    Args:
        query (str): æœç´¢å…³é”®è¯
    
    Returns:
        dict: AIåˆ†ææ•°æ®
    """
    # æŸ¥æ‰¾æœ€æ–°çš„AIåˆ†ææ–‡ä»¶
    pattern = f"processed_data/06_ai_processed_data/{query}_analyzed_*.json"
    files = glob.glob(pattern)
    
    if not files:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æŸ¥è¯¢ '{query}' çš„AIåˆ†ææ–‡ä»¶")
    
    # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
    latest_file = max(files, key=os.path.getctime)
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        return json.load(f)

def generate_basic_summary(query, analyzed_data):
    """
    ç”ŸæˆåŸºç¡€æ±‡æ€»æŠ¥å‘Šï¼ˆä¸ä¾èµ–AIåˆ†æï¼‰

    Args:
        query (str): æœç´¢å…³é”®è¯
        analyzed_data (dict): åˆ†ææ•°æ®

    Returns:
        dict: åŸºç¡€æ±‡æ€»æŠ¥å‘Š
    """
    news_items = analyzed_data.get('news_items', [])
    total_news = len(news_items)

    # æŒ‰è¯­è¨€åˆ†ç±»
    language_stats = {}
    for item in news_items:
        lang = item.get('detected_language', 'unknown')
        language_stats[lang] = language_stats.get(lang, 0) + 1

    # æå–æ–°é—»æ ‡é¢˜å’Œæ‘˜è¦
    news_summaries = []
    for item in news_items:
        summary = {
            "id": item.get('id'),
            "title": item.get('title', ''),
            "url": item.get('url', ''),
            "language": item.get('detected_language', 'unknown'),
            "content_extracted": item.get('content_extracted', False),
            "content_length": len(item.get('content', '')) if item.get('content') else 0
        }
        news_summaries.append(summary)

    # ç”ŸæˆåŸºç¡€åˆ†æ
    basic_analysis = f"""åŸºäºæœç´¢å…³é”®è¯"{query}"çš„æ–°é—»åˆ†ææŠ¥å‘Šï¼š

æœ¬æ¬¡åˆ†æå…±æ”¶é›†åˆ°{total_news}æ¡ç›¸å…³æ–°é—»ã€‚

æ–°é—»æ¥æºè¯­è¨€åˆ†å¸ƒï¼š
{chr(10).join([f"- {lang}: {count}æ¡" for lang, count in language_stats.items()])}

ä¸»è¦æ–°é—»æ ‡é¢˜ï¼š
{chr(10).join([f"- {item['title']}" for item in news_summaries[:10]])}

è¿™äº›æ–°é—»æ¶µç›–äº†{query}ç›¸å…³çš„æœ€æ–°åŠ¨æ€å’Œå‘å±•è¶‹åŠ¿ï¼Œä¸ºäº†è§£è¯¥ä¸»é¢˜æä¾›äº†å…¨é¢çš„ä¿¡æ¯è§†è§’ã€‚"""

    # æ„å»ºåŸºç¡€æŠ¥å‘Š
    basic_report = {
        'query': query,
        'final_summary': basic_analysis,
        'generated_time': datetime.now().isoformat(),
        'analysis_type': 'basic_summary',
        'total_news_analyzed': total_news,
        'language_distribution': language_stats,
        'news_summaries': news_summaries
    }

    return basic_report

def extract_successful_analyses(analyzed_data):
    """
    æå–æˆåŠŸåˆ†æçš„å†…å®¹
    
    Args:
        analyzed_data (dict): AIåˆ†ææ•°æ®
    
    Returns:
        tuple: (successful_analyses, analysis_summary)
    """
    successful_analyses = []
    failed_count = 0
    
    for news_item in analyzed_data['news_items']:
        if news_item.get('analysis_success', False):
            analysis_text = news_item.get('analyzed', '')
            if analysis_text and len(analysis_text.strip()) > 20:
                # æ·»åŠ æ–°é—»æ ‡é¢˜ä½œä¸ºä¸Šä¸‹æ–‡
                formatted_analysis = f"ã€{news_item['title']}ã€‘\n{analysis_text}"
                successful_analyses.append(formatted_analysis)
        else:
            failed_count += 1
    
    analysis_summary = {
        'total_items': len(analyzed_data['news_items']),
        'successful_analyses': len(successful_analyses),
        'failed_analyses': failed_count
    }
    
    return successful_analyses, analysis_summary

def generate_final_summary(query, analyzed_data):
    """
    ç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
    
    Args:
        query (str): æœç´¢å…³é”®è¯
        analyzed_data (dict): AIåˆ†ææ•°æ®
    
    Returns:
        dict: æœ€ç»ˆæ±‡æ€»æŠ¥å‘Šæ•°æ®
    """
    print("  æ­£åœ¨æå–æˆåŠŸçš„åˆ†æç»“æœ...")
    successful_analyses, analysis_summary = extract_successful_analyses(analyzed_data)
    
    if not successful_analyses:
        print("  æ²¡æœ‰æˆåŠŸçš„AIåˆ†æç»“æœï¼Œç”ŸæˆåŸºç¡€æ±‡æ€»æŠ¥å‘Š...")
        return generate_basic_summary(query, analyzed_data)
    
    print(f"  æ‰¾åˆ° {len(successful_analyses)} ä¸ªæˆåŠŸçš„åˆ†æç»“æœ")
    
    # æ‹¼æ¥æ‰€æœ‰åˆ†ææ–‡æœ¬
    all_analyzed_texts = '\n\n'.join(successful_analyses)
    
    # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œè¿›è¡Œæˆªæ–­ï¼ˆä¿ç•™å‰é¢çš„å†…å®¹ï¼‰
    max_length = 15000  # GLM-4.5-flashå¯ä»¥å¤„ç†æ›´é•¿æ–‡æœ¬ï¼Œæé«˜é™åˆ¶
    if len(all_analyzed_texts) > max_length:
        print(f"  åˆ†ææ–‡æœ¬è¿‡é•¿({len(all_analyzed_texts)}å­—ç¬¦)ï¼Œæˆªæ–­åˆ°{max_length}å­—ç¬¦")
        all_analyzed_texts = all_analyzed_texts[:max_length] + "...\n\n[æ³¨ï¼šç”±äºå†…å®¹è¿‡é•¿ï¼Œéƒ¨åˆ†åˆ†æå·²çœç•¥]"
    
    print("  æ­£åœ¨è°ƒç”¨AIç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š...")
    
    # è°ƒç”¨AIç”Ÿæˆæ±‡æ€»æŠ¥å‘Šï¼ˆå…ˆå°è¯•GLMï¼Œå¤±è´¥åˆ™å°è¯•DeepSeekï¼‰
    success, summary_result, error_msg = call_glm_summary_api(query, all_analyzed_texts)
    
    if not success:
        print(f"âš ï¸ GLM APIè°ƒç”¨å¤±è´¥: {error_msg}")
        print("ğŸ”„ å°è¯•ä½¿ç”¨DeepSeekä½œä¸ºå¤‡é€‰...")
        
        # å°è¯•DeepSeek API
        success, summary_result, deepseek_error = call_deepseek_summary_api(query, all_analyzed_texts)
        
        if not success:
            print(f"âŒ DeepSeek APIä¹Ÿå¤±è´¥: {deepseek_error}")
            raise Exception(f"æ‰€æœ‰AI APIéƒ½å¤±è´¥ - GLM: {error_msg}, DeepSeek: {deepseek_error}")
        else:
            print("âœ… DeepSeek APIè°ƒç”¨æˆåŠŸï¼")
    else:
        print("âœ… GLM APIè°ƒç”¨æˆåŠŸï¼")
    
    # æ„å»ºæœ€ç»ˆæŠ¥å‘Šæ•°æ®
    final_report = {
        'query': query,
        'final_summary': summary_result,
        'generated_time': datetime.now().isoformat(),
        'source_analysis_count': len(successful_analyses),
        'total_news_analyzed': analysis_summary['total_items'],
        'analysis_statistics': analysis_summary,
        'generation_model': os.getenv('GEMINI_MODEL_ID', 'gemini-2.0-flash-exp'),
        'workflow_summary': {
            'step1_raw_news': analyzed_data.get('total_count', 0),
            'step2_accessible': analyzed_data.get('accessibility_check', {}).get('accessible_count', 0),
            'step3_relevant': analyzed_data.get('ai_relevance_filter', {}).get('filtered_count', 0),
            'step4_html_fetched': analyzed_data.get('total_count', 0),
            'step5_content_extracted': analyzed_data.get('content_extraction', {}).get('extracted_count', 0),
            'step6_analyzed': analysis_summary['successful_analyses'],
            'step7_final_summary': 1 if success else 0
        }
    }
    
    return final_report

def save_final_summary(query, final_report):
    """
    ä¿å­˜æœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
    
    Args:
        query (str): æœç´¢å…³é”®è¯
        final_report (dict): æœ€ç»ˆæ±‡æ€»æŠ¥å‘Šæ•°æ®
    
    Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = 'processed_data/07_final_summary_reports'
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{query}_summary_{timestamp}.json"
    filepath = os.path.join(output_dir, filename)
    
    # ä¿å­˜æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, ensure_ascii=False, indent=2)
    
    return filepath

def step7_generate_final_summary(query):
    """
    æ­¥éª¤7ä¸»å‡½æ•°ï¼šç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
    
    Args:
        query (str): æœç´¢å…³é”®è¯
    
    Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    print(f"[Step 7/7] Generating final summary report...")
    
    # åŠ è½½AIåˆ†æç»“æœ
    print("  æ­£åœ¨åŠ è½½AIåˆ†æç»“æœ...")
    analyzed_data = load_analyzed_data(query)
    
    # ç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
    final_report = generate_final_summary(query, analyzed_data)
    
    # ä¿å­˜ç»“æœ
    filepath = save_final_summary(query, final_report)
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"  æ­¥éª¤7å®Œæˆï¼æœ€ç»ˆæ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ")
    if 'workflow_summary' in final_report:
        workflow_stats = final_report['workflow_summary']
        print(f"  å·¥ä½œæµç»Ÿè®¡:")
        print(f"    åŸå§‹æ–°é—»: {workflow_stats['step1_raw_news']}")
        print(f"    å¯è®¿é—®: {workflow_stats['step2_accessible']}")
        print(f"    ç›¸å…³ç­›é€‰: {workflow_stats['step3_relevant']}")
        print(f"    å†…å®¹æå–: {workflow_stats['step5_content_extracted']}")
        print(f"    æˆåŠŸåˆ†æ: {workflow_stats['step6_analyzed']}")
    else:
        print(f"  åŸºç¡€æ±‡æ€»æŠ¥å‘Šç»Ÿè®¡:")
        print(f"    æ€»æ–°é—»æ•°: {final_report.get('total_news_analyzed', 0)}")
        print(f"    åˆ†æç±»å‹: {final_report.get('analysis_type', 'unknown')}")
    print(f"  æœ€ç»ˆæŠ¥å‘Šé•¿åº¦: {len(final_report['final_summary'])} å­—ç¬¦")
    print(f"  ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
    
    return filepath

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    test_query = "è‹±ä¼Ÿè¾¾"
    try:
        result_file = step7_generate_final_summary(test_query)
        print(f"æµ‹è¯•å®Œæˆï¼Œç»“æœæ–‡ä»¶: {result_file}")
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
